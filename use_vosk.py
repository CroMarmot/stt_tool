# uv add vosk pydub typer rich
import os
import sys
from pathlib import Path
from typing import Optional
import tempfile
import time

import typer
from pydub import AudioSegment
from vosk import Model, KaldiRecognizer
import json

# rich ç”¨äºç¾åŒ–è¾“å‡º
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.traceback import install

install()  # ç¾åŒ–å¼‚å¸¸ traceback
console = Console()

app = typer.Typer(help="ä½¿ç”¨æœ¬åœ° Vosk æ¨¡å‹å°†éŸ³é¢‘è½¬ä¸ºæ–‡å­—ï¼ˆæ”¯æŒ SRT æ—¶é—´è½´ï¼‰")

# =============================================
# ğŸ”§ å¯é…ç½®é¡¹ï¼ˆç”¨æˆ·å¯ä¿®æ”¹ï¼‰
# =============================================

# æ¨¡å‹æ–‡ä»¶å¤¹åç§°ï¼ˆæ”¾åœ¨ç¨‹åºåŒçº§ç›®å½•æˆ–ç»å¯¹è·¯å¾„ï¼‰
MODEL_FOLDER = "vosk-model-cn-0.22"  # æ¨èä½¿ç”¨ 0.22 æˆ–æ›´å¤§æ¨¡å‹

# ä¸´æ—¶æ–‡ä»¶å‰ç¼€
TEMP_PREFIX = "vosk_transcribe_"

# é‡‡æ ·ç‡
SAMPLE_RATE = 16000

# æ ‡ç‚¹æ¢å¤æ¨¡å‹ï¼ˆæ¨èè½»é‡çº§ï¼‰
PUNCTUATOR_MODEL = "cantonese-zh/punc_ctc_zh_base"

# =============================================
# è‡ªåŠ¨ç”Ÿæˆä¸´æ—¶ WAV è·¯å¾„
# =============================================


def get_temp_wav_path() -> Path:
    timestamp = int(time.time())
    temp_name = f"{TEMP_PREFIX}{timestamp}.wav"
    temp_path = Path(tempfile.gettempdir()) / temp_name
    return temp_path


TEMP_WAV_PATH = get_temp_wav_path()

# =============================================
# æ—¶é—´æ ¼å¼åŒ–ï¼šç§’ â†’ SRT æ—¶é—´
# =============================================


def time_format(seconds: float) -> str:
    m, s = divmod(seconds, 60)
    h, m = divmod(m, 60)
    return f"{int(h):02}:{int(m):02}:{int(s):02},{int((s % 1) * 1000):03}"


# =============================================
# ä¸­æ–‡æ ‡ç‚¹æ¢å¤ï¼ˆåŸºäº Hugging Face BERTï¼‰
# =============================================

try:
    from transformers import AutoTokenizer, AutoModelForTokenClassification
    import torch

    HAS_PUNCTUATOR = True
except ImportError:
    HAS_PUNCTUATOR = False


class ChinesePunctuator:
    def __init__(self, model_name: str = PUNCTUATOR_MODEL):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForTokenClassification.from_pretrained(model_name)
        self.id2label = self.model.config.id2label

    def punctuate(self, text: str) -> str:
        words = text.strip().split()
        if not words:
            return text

        inputs = self.tokenizer(
            words,
            is_split_into_words=True,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512,
        )

        with torch.no_grad():
            logits = self.model(**inputs).logits
        predictions = torch.argmax(logits, dim=-1)[0].numpy()

        result = ""
        for i, word in enumerate(words):
            result += word
            label_id = predictions[i + 1]  # æ³¨æ„ï¼š[CLS] åœ¨å¼€å¤´
            label = self.id2label[label_id]
            if label in ["ï¼Œ", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼š"]:
                result += label
            else:
                result += " "  # ä¿æŒè¯è¯­è¿æ¥è‡ªç„¶
        return result.strip()


# å…¨å±€æ ‡ç‚¹æ¢å¤å™¨ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
_punctuator = None


def restore_punctuation(text: str) -> str:
    global _punctuator
    if not text.strip():
        return text

    try:
        if _punctuator is None:
            console.print("[cyan]ğŸ“¥ æ­£åœ¨åŠ è½½ä¸­æ–‡æ ‡ç‚¹æ¢å¤æ¨¡å‹...[/cyan]")
            _punctuator = ChinesePunctuator()
        return _punctuator.punctuate(text)
    except Exception as e:
        console.print(f"[yellow]âš ï¸ æ ‡ç‚¹æ¢å¤å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡ï¼š{e}[/yellow]")
        return text  # å¤±è´¥æ—¶è¿”å›åŸæ–‡


# =============================================
# ä¸»å‘½ä»¤
# =============================================


@app.command()
def transcribe(
    audio_path: Path = typer.Argument(..., help="è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„"),
    model_path: Optional[Path] = typer.Option(
        MODEL_FOLDER, "--model", "-m", help=f"Vosk æ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤: {MODEL_FOLDER}ï¼‰"
    ),
):
    """
    å°†éŸ³é¢‘æ–‡ä»¶è½¬ä¸ºå¸¦æ ‡ç‚¹çš„æ–‡æœ¬å’Œ SRT å­—å¹•
    """
    if not audio_path.exists():
        console.print(f"[red]âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{audio_path}[/red]")
        raise typer.Exit(1)

    model_path = model_path or Path(MODEL_FOLDER)
    txt_path = audio_path.with_suffix(audio_path.suffix + ".txt")
    srt_path = audio_path.with_suffix(audio_path.suffix + ".srt")

    console.print(f"[bold green]ğŸ™ï¸ å¼€å§‹å¤„ç†éŸ³é¢‘ï¼š{audio_path}[/bold green]")
    console.print(f"[cyan]æ¨¡å‹è·¯å¾„ï¼š{model_path}[/cyan]")

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
        transient=True,
    ) as progress:
        try:
            # æ£€æŸ¥æ¨¡å‹
            if not model_path.exists():
                console.print(f"[red]âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼š{model_path}[/red]")
                console.print(
                    "[yellow]è¯·ä¸‹è½½ä¸­æ–‡æ¨¡å‹ï¼šhttps://alphacephei.com/vosk/models[/yellow]"
                )
                raise typer.Exit(1)

            # Step 1: åŠ è½½æ¨¡å‹
            progress.add_task(description="åŠ è½½ Vosk æ¨¡å‹...", total=None)
            model = Model(str(model_path))
            rec = KaldiRecognizer(model, SAMPLE_RATE)

            # Step 2: è½¬æ¢éŸ³é¢‘
            progress.remove_task(list(progress._tasks.keys())[-1])
            progress.add_task(description="è½¬æ¢éŸ³é¢‘æ ¼å¼ä¸º 16kHz WAV...", total=None)
            audio = AudioSegment.from_file(str(audio_path))
            audio = audio.set_channels(1).set_frame_rate(SAMPLE_RATE)
            audio.export(str(TEMP_WAV_PATH), format="wav")

            # Step 3: è¯­éŸ³è¯†åˆ«
            progress.remove_task(list(progress._tasks.keys())[-1])
            progress.add_task(description="æ­£åœ¨è¯†åˆ«è¯­éŸ³...", total=None)
            results = []
            with open(TEMP_WAV_PATH, "rb") as f:
                f.read(44)  # è·³è¿‡ WAV å¤´
                while True:
                    data = f.read(4000)
                    if len(data) == 0:
                        break
                    if rec.AcceptWaveform(data):
                        results.append(rec.Result())
                results.append(rec.FinalResult())

            segments = []
            for res in results:
                res_json = json.loads(res)
                if "text" in res_json and res_json["text"].strip():
                    segments.append(res_json)

            if not segments:
                console.print("[yellow]âš ï¸ æœªè¯†åˆ«å‡ºä»»ä½•æ–‡å­—[/yellow]")
                raise typer.Exit(0)

            total_duration = len(audio) / 1000.0  # ç§’

            # Step 4: å†™å…¥ TXTï¼ˆå¸¦æ ‡ç‚¹ï¼‰
            progress.remove_task(list(progress._tasks.keys())[-1])
            progress.add_task(description="ç”Ÿæˆå¸¦æ ‡ç‚¹æ–‡æœ¬æ–‡ä»¶...", total=None)
            with open(txt_path, "w", encoding="utf-8") as f:
                for seg in segments:
                    clean_text = restore_punctuation(seg["text"])
                    f.write(clean_text + "\n")
            console.print(f"[bold]âœ… æ–‡æœ¬å·²ä¿å­˜ï¼š{txt_path}[/bold]")

            # Step 5: å†™å…¥ SRT
            progress.remove_task(list(progress._tasks.keys())[-1])
            progress.add_task(description="ç”Ÿæˆå­—å¹•æ–‡ä»¶...", total=None)
            with open(srt_path, "w", encoding="utf-8") as f:
                for i, seg in enumerate(segments, 1):
                    text = seg["text"]
                    clean_text = restore_punctuation(text)

                    if "result" in seg and seg["result"]:
                        # ç²¾ç¡®æ—¶é—´æˆ³
                        words = seg["result"]
                        start_sec = words[0]["start"]
                        end_sec = words[-1]["end"]
                    else:
                        # å…œåº•ï¼šå¹³å‡åˆ†é…æ—¶é—´
                        seg_duration = total_duration / len(segments)
                        start_sec = (i - 1) * seg_duration
                        end_sec = i * seg_duration

                    start = time_format(start_sec)
                    end = time_format(end_sec)
                    f.write(f"{i}\n{start} --> {end}\n{clean_text}\n\n")

            console.print(f"[bold]âœ… SRT å­—å¹•å·²ä¿å­˜ï¼š{srt_path}[/bold]")

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if TEMP_WAV_PATH.exists():
                os.unlink(TEMP_WAV_PATH)
                console.print(f"[dim]ğŸ—‘ï¸ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†ï¼š{TEMP_WAV_PATH}[/dim]")

            console.print("[bold green]ğŸ‰ è½¬å½•å®Œæˆï¼[/bold green]")

        except Exception as e:
            console.print(f"[red]âŒ å¤„ç†å¤±è´¥ï¼š{str(e)}[/red]")
            if TEMP_WAV_PATH.exists():
                os.unlink(TEMP_WAV_PATH)
            raise typer.Exit(1)


if __name__ == "__main__":
    app()
